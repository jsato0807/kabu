kabu_montecarlo.pyが終わったら、kabu_backtest.pyを、cumulativeなreturnをtxtfileに保存するように修正して、再度kabu_montecarlo.pyを実行せよ
svmとGAの論文を読んで、完全に再現
svmとGAの論文で使われている、動的なGAについて、ハイパーパラメータをどう決めるか、その動的な手法が高精度なのはどのような原因が考えられるか解読する
svmとGAの論文以外にcitationの多い、overfitしていない先行研究を探す→Modeling, forecasting and trading the EUR exchange rates with hybrid rolling genetic algorithms-Support vector regression forecast combinations（citation132）
kabu_montecarlo.pyはパラメータを全て一様分布から生成しているが、これも先行研究を探す
covariance penalitesの論文にある、CV以外の先行研究となった手法を調べる
cpcvを日足より短い”大量の”データを使って再度試す
うまくいきそうなら、その結果に対してcscvでpboを確認
transaction costsを導入すべきか考える、また最適化する関数の返り値がeffective_marginでは、過学習するからリスクを考慮した指標を使用するべきで、sharp_ratioか、最大ドローダウンなどを組み合わせた独自の関数を作るか考える→遺伝プログラミングで評価関数を自動で生成すれば良いのではないか


・各戦略が生成したリターンの統計的な性質に焦点を当てた論文

A Bayesian Approach to Measurement of Backtest Overfitting (2021年のものなのでまだcitationが少ない)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66e2115f-c26c-800e-ac38-9ebc1d24fa2e
指定した平均と標準偏差から複数の投資戦略のリターンを生成し、MCMCを使って戦略の平均と共分散をサンプリング。その後、その平均と共分散からout_sampleリターンを生成し、シャープレシオを計算。




・各戦略のパラメータを具体的に最適化することに焦点を当てた論文
    <GA関連>
Parameter Optimization for Trading Algorithms of Technical Agents
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66e214f0-9a78-800e-a1cc-f45886fd52c0
BOとGAを使って累積リターンが最大となるようなRSIの下限値と上限値、ボリンジャーバンドの平均と標準偏差を推定
ロールフォワード法によるoverfit軽減

FOREX Trading Strategy Optimization (citationほぼ0)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66dc4040-d17c-800e-aada-7f882fe2e79c
GAを使って移動平均の次数、取引頻度のパラメータ、最小閾値を最適化
trainとtestを分けただけ



    <svm関連>
Combining Support Vector Machine with Genetic Algorithms to optimize investments in Forex markets with high leverage (citation77)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66de16a3-db5c-800e-8c3a-d8228ae1bd8b
svmにより、市場のトレンドを３つに判別し、各トレンドに対応するGAで別々に最適化、GAはハイパーミューテーション、ハイパーセレクションを実行
svmに対してkfold交差検証を実施、GAに対してはtrainとtestを分けただけ

Modeling, forecasting and trading the EUR exchange rates with hybrid rolling genetic algorithms-Support vector regression forecast combinations (citation132)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66df6ac1-189c-800e-ba55-fd18a8a7e128
svrによって過去の為替レートから１日先の予測値を算出、GAでsvrのハイパーパラメータを調整。svrの入力データは教師データ：ARMAモデルやNNによって得られる１日先の予測値、教師ラベル：実際の１日先の値、として学習。GAの評価関数は、年率リターン-10*RMSE-0.001*(サポートベクターの数/トレーニングサンプル数)
svrに含まれる正則化項Cによって過学習を抑えられる
※為替データをそのまま与えるとoverfitしてしまうが、例えば移動平均線の値を与えると、入力データが平均化されていることで、overfitしにくくなるのではないか



<その他>
Evaluating machine learning classification for financial trading: An empirical approach (citation200)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66e0bb4c-2f04-800e-981e-684f82061d9e
シンプルな機械学習モデル（OneR, C4.5, JRip, Logistic Model Tree, KStar, Naïve Bayes）を使用し、予測精度が50%程度だったが高い収益性を誇った。異なる市場でも安定した結果となった。シンプルなモデルのため計算コストが低いことを活かして定期的に再訓練ができる

Data selection to avoid overfitting for foreign exchange intraday trading with machine learning
https://www.perplexity.ai/search/data-selection-to-avoid-overfi-NQKC0.7bQuqe17n3LDPRdA
新しくパスロス指標(精度＊log(正規化されたinsampleリターン)/収益率)を提案、２つの学習目的（回帰と分類）、２つの取引戦略（１機関保有、切り替えまで保有）、４つの機械学習モデルを使用（NN、ランダムフォレスト、SVM/SVR、XGBoost）


・新しい手法
Avoiding Backtesting Overfitting by Covariance-Penalties: an empirical investigation of the ordinary and total least squares cases (citation12)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66da70eb-4eb0-800e-a45b-4825778a7845
共分散ペナルティ補正法によるoverfitの回避




・交差検証の応用
The Probability of Backtest Overfitting (citation132)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66dba0a1-bcec-800e-9da5-89196b3a70b5
cscv法によるPBOの計算によりoverfitを定量的に評価できる

effective_margin+{(sharp_ratio-1)-effective_margin*max_draw_down}*effective_margin


・kabu_backtestで通貨ペアの二カ国のいずれかの祝日に被っている日は決済できないので、それをposition closure processingに追加で実装せよ
また、二カ国両方とも祝日の場合はどうなるか確認せよ
→swap pointの受け渡しができないだけで、祝日はfxの取引は通常通り行えるので、修正不要

・kabu_swap.pyを、minkabuだけでなくoandaからもスワップポイントをスクレイピングできるようにしつつ、oandaのサイトには掲載がない2019年4月より前のデータに対しては比率を計算して、理論値を算出するように実装せよ。

kabu_compare_intrestrate_and_oandascraping.py内で、calculate_swap_averages関数を呼び出す際に指定されるcurrent_startとcurrent_endの範囲で、kabu_oanda_swapscraping.pyで生成されるcsvファイルがあるかを判定してしまってる。
範囲を修正せよ。→解決

その後、average_buy_swap、average_sell_swapがそれぞれ０と２になってしまう原因を追求せよ
→解決

2024-11-03
kabu_compare_intrestrate_and_oandascraping.pyでほぼ正しい結果が出たが、2021-12~2022-02にかけての平均スワップの値が飛んでしまっているので、計算エラーかを確認せよ。


2024-11-09
kabu_swap.pyの__init__内のif found_file以下が正常に動作するかをkabu_backtest.pyを走らせて確かめる(つまりwebsite="oanda",interval="M1",link=link)
→確認完了

kabu_backtest.pyのlong_onlyのcheck swap　以下を修正したので、動作確認をする（つまり、website="minkabu",interval="1d"）
→long_onlyのcheck swap以下にpos[8] += ...を追加し、check_min_max_effective_marginを追加したことで変わるのは、シャープレシオと最大ドローダウンだけなので、check totalの値と有効証拠金の値の一致不一致には関係ない
動作確認は完了

website="oanda"でlong_onlyの時、計算が合わないようなので、確認（つまりまずはwebsite="minkabu",strategy="long_only"で動作確認）
→website="minkabu"では合うことを確認、website="oanda"では計算が合わないことを確認

→おそらく原因は、スクレイピングで得たデータは日本の日付でのスワップデータなので、それを世界基準に直さないとスワップポイントの計算でずれが生まれるのではないか


oanda証券のスワップカレンダーに記載されているスワップポイントの単位を調べよ

2024-11-10
kabu_swap.pyの以下の部分を追加で実装せよ

        # データ取得の制限を確認
        if start_date < datetime(2019, 4, 1):
            print("2019年4月以前のデータはありません。")
            start_date = datetime(2019, 4, 1)

2024-11-12
2024-11-09での確認結果と異なり、websiteやpairの値に関わらず計算が合わない場合があることを確認
Nov 2, 9のコミットでは、Nov 10とは異なり、計算が一致
Nov 9、でEURGBP website=oandaの時のみ計算の不一致を確認（初期条件はinterval=1d,initial_funds=2000000, grid_start=0.84, grid_end=0.94, ordersize=3000, num_traps_options = 100, profit_width=100）この条件では強制ロスカットが作動するが、website=oandaの時のみ計算が合わない
→kabu_swap.pyのget_total_swap_pointsのif self.website==oanda以下のwhile current <= current_dateの部分で、イコールが不要でさらにopen_dateからcurrent_dateまでの営業日数分だけスワップポイントを取得して足し合わせる必要がある。よってwhile current < open_date+timedelta(days=rollover_days) とするのが適切。この修正により、初期条件は(start_date=2021-01-04 end_date=2021-04-01,interval=1d,initial_funds=100000, grid_start=0.86, grid_end=0.91, ordersize=3000, num_traps_options = 100, profit_width=100) この初期条件で強制ロスカットが作動するが計算が一致することを確認
→while current < open_date+timedelta(days=rollover_days)　ではスワップポイントが０になってしまうので不適

Nov 9の時点で、
    website=oandaでかつM1のデータの場合に、ロスカットする場合もそうでない場合も計算が一致することを確認せよ→次に記す初期条件で強制ロスカット時も計算が合うことを確認(pair="EURGBP=X" interval=M1 website=oanda start_date=2021-01-04 end_date=2021-02-01 initial_funds=100000 grid_start=0.86 grid_end=0.91 strategies=long_only order_sizes=10000 num_traps_options=100 profit_width=100)
    →website=oandaでかつM1のデータの場合に、ロスカットする場合は計算が一致することを確認（interval=1dでも確認）
    kabu_swap.pyのif self.website == oanda以下の、返り値の部分で、oanda証券のサイトにはロールオーバーを考慮したスワップポイントが記載されているため、サイトにデータのある2019/04以降のデータでは、返り値はこの値で良いが、2019/04以前のデータはないため、自分でスワップポイントを計算する際はrollover_daysをかけないとだめ
    get_data_rangeメソッドで、dateとcurrent_startが完全に一致しないとstart_collectingフラグがTrueにならないので、たとえばEURGBPの2021/01/01のデータはないので、current_startをその日にするとダメ、修正せよ


2024-11-14
get_holidays_from_pairメソッド内でholidays.CountryHolidaysで指定した国の祝日をdict型に収納し、__init__以下でself.each_holidaysに保有させているが、祝日が日本語になってしまうため、multiprocessingによるpickle化ができない。よって、CountryHoliday().keys()のみをholidays_dictに保有させることで、祝日の名前を除外して日付のみを保有でき問題が解決する

2024-11-16
改めてkabu_swap.pyのget_total_swap_points内のwhile文を修正し、interval="1d"の時はwebsite="oanda"で計算が一致することを確認
→interval="M1"でロスカットが起こる場合(pair="EURGBP=X" interval=M1 website=oanda start_date=2021-01-04 end_date=2021-04-01 initial_funds=100000 grid_start=0.86 grid_end=0.91 strategies=long_only order_sizes=10000 num_traps_options=100 profit_width=100)に計算が一致するか確認中
 またロスカットが起きない場合も確認(pair="EURGBP=X" interval=M1 website=oanda start_date=2021-01-04 end_date=2021-02-01 initial_funds=10000000 grid_start=0.86 grid_end=0.91 strategies=long_only order_sizes=1000 num_traps_options=100 profit_width=100)

 website="oanda"の時は日付が日本になっているので、現地時間に直す必要がある

 2024-11-17
 kabu_backtest.pyで次の初期条件で計算の不一致を確認(pair: "AUDNZD=X", interval: 1d, website:minkabu, start_date:2019-11-01 00:00:00, end_date:2019-11-30 00:00:00, initial_funds:100000000, grid_start:1.02, grid_end:1.14, strategies:['long_only'], entry_intervals:[0], total_thresholds:[10000], order_sizes:[1000],num_trap_options:[100], profit_widths:[0.01], densities:[10])

 2024-11-19
 kabu_backtest.pyで上記の初期条件でinitial_fund=100000にすると計算エラーが発生することを確認
 →check swapをコメントアウトすると計算が一致することからスワップポイントの計算が原因であることがわかった
 →結局は、swapではなく、なぜかlong_onlyの時だけfor i in range(len(data))の次の、if margin_maintenance_flagの部分が、if margin_maintenance_flag or order_capacity_flag:になっていた。
  これだと、order_marginが０以下になったときに、ここでbreakしてしまうことで為替の変動に伴って発生する、すでに保有しているポジションの価格変動やスワップポイントの変動もしないままになるので、結果が合わなくなる

以上を踏まえて以前試した、計算結果が一致しない条件で再度確認(EURGBP=X, interval: M1, website:oanda, start_date:2021-01-04 00:00:00, end_date:2021-04-01 00:00:00, initial_funds:100000, grid_start:0.86, grid_end:0.91, strategies:['long_only'], entry_intervals:[0], total_thresholds:[1000], order_sizes:[10000],num_trap_options:[100], profit_widths:[100], densities:[10])

2024-11-21
上記の条件で確認したところ、不一致が確認された。なお2021-01-04 17:55:00 でロスカットされる。
試しに、check swapの部分をコメントアウトし、最後のswap_valueの計算を0としたところ、ロスカットが実行されていても一致
→ロスカットが実行された場合の、最後のswap_valueの計算が間違っているのではないか。

あとは、for i in range(len(data))とあり、dataの量が非常に多いと計算が重くなってしまう
→バッチ処理はどうか

そもそも、2024-11-19の最後に記した初期条件では、2021-01-04から始まり、2021-01-04 17:55:00でロスカットが執行されるが、website==oandaの時には、rollover_daysが0であるにも関わらず、スワップポイントが付与されてしまう仕様になっていたので、
if rollover_days== 0 と if rollover_days >= 1を追加して、0の時はそもそも計算などせずに0を返すようにkabu_swap.pyを修正した
しかし、その修正前に起きた計算の不一致の本質的な解決にはおそらくなっていない。つまり、特定の条件下でのみ計算の不一致が生じる問題の解決にはなっていない。
→とりあえず、修正したkabu_swap.pyで2024-11-19の最期に記した初期条件で、計算が一致するか確認

2024-11-23
kabu_swap.pyで、ロールオーバーの計算はadd_business_daysメソッドにより、ニューヨーク時間に直してから計算してるが、get_total_swap_pointsでwebsite=oandaの時に、self.swap_points_dictからスワップポイントのデータを取得する際に元となっているoanda証券のサイトでは、日付が日本になっているため、齟齬が生じる

2024-11-28
kabu_swap.pyをtest_get_holidays.pyに基づいて大幅に修正したが、正常に動作しないのでデバッグせよ

2024-11-30
kabu_swap.pyで、self.business_holidaysを作る際に、祝日を１年単位で取得した後に、それに含まれない時間帯をbusiness_daysとしているが、rolloverを計算する際に２営業日後の情報が必要
なので、12/31など年末ギリギリのデータが来たときに、その２営業日後が正しく計算できない可能性があるので、修正せよ

2024-12-1
ロスカットが実行される場合にswap_valueが正しく計算されない模様。値が0になってしまうので、おそらくswap_value計算時のget_total_swap_pointsメソッド周りでエラーがあると思われる

2024-12-3
    kabu_swap.pyのget_total_swap_pointsメソッドで、ロールオーバーを計算する前に、open_dateとcurrent_dateを、pairで指定される二カ国の現地時間でのNYクローズと比較することで、それを跨いでいない場合はそもそもロールオーバーは０に決まっているので計算する必要がない、という条件分岐を追加せよ
    →crossover_ny_closeメソッドをSwapCalculatorに追加した。
    https://www.oanda.jp/course/ty3/swap が見れなくなっているので、ny4に移行したほうがいいかも。さらに２つの違いの詳細を把握せよ
    →oanda証券のミスだったので、現在は復旧したためとりあえず問題なし。
    →pairの右側の単位でスワップポイントが表示されている(例えば、USDJPYならば円)ので、単位を合わせよ。

2024-12-5
    2019-4-1以前ではスワップカレンダーがないので、ScrapeFromOanda内で、計算するメソッドを定義し、その関数をSwapCalculatorのinitのところでwebsite=="theory"で動作するようにもせよ


2024-12-7
    kabu_compare_bis_intrestrate_and_oandascraping.py で累積平均や、移動平均の部分でもtheoryを普通の平均で計算してしまっているので、修正
    →このままで問題なし

2024-12-10
    scrape_from_oandaの手前で、found_fileの名称を決めているが、例えばend_date = 2024-10-31の時に、file名が2024-10-30のようにその1日前であっても、最初からscrapingしてしまうので、近い日付があればscrapeするのはその続きからになるように修正せよ
    また、kabu_oanda_swapscrapingのscrape_from_oandaと、kabu_swap.py内のScrapeFromOanda内のscrape_from_oandaメソッドを統一した方が良いのでは？

2024-12-12
    kabu_library.py のget_swap_points_dictメソッドのmissing_rangesに2019-4-1以前の範囲が指定された場合、その前段階ですでにファイルにある範囲のデータをダウンロードしているにも関わらず、scrape_from_oanda関数内でstart_date=2019-4-1,end_date=datetime.now(jst)としてしまうので、またこの範囲でスクレイピングが実行されてしまう。
    →get_swap_points_dictの最初で2019-4-1以前であれば、start_date=2019-4-1,end_date=datetime.now()とすることで、あらかじめ理論値計算に必要な、実際の値を可能な限りスクレイピングすることで解決

2024-12-14
    kabu_swap.pyで2019-4-1以前の場合にswap_points_dictを獲得すると、2019-3-30が土曜日にも関わらず、スワップポイントの値が０でない状態になる。
    おそらくkabu_compare_bis_intrestrate_and_oandascraping.pyのcalculate_theory_swap内の辞書output_dataを作成する際に時差の関係で、日付がずれてしまい、本来金曜日の値のはずが土曜日30日のスワップポイントとして表示されていると思われる
    →kabu_swap.pyのrollover_daysの計算を修正することで、平日のみを考慮して解決
    しかし、その部分の計算が非常に遅いので、要修正

    また、kabu_library.pyのget_data_range関数が、current_startやcurrent_endに休日を指定してしまうと、データがないため適切に範囲指定できない
    →current_start,current_endに最も近い日付にずらすようなアルゴリズムを追加実装せよ

2024-12-15
    前日の課題２つ（rollover_daysの計算、get_data_range)が未解決
    →1つ目はkabu_swap.pyを過去に戻って逐一営業日判定する方法の方が早いかも。比較せよ。

2024-12-17
    結局rollover_daysをwebsite ==oandaの時は計算しないことで、大幅な時間短縮には成功した。
    しかし、計算の不一致が発生し、kabu_swap.pyを遡っても原因不明
    →check swap 以下でget_total_swap_pointsでスワップポイントを計算する際に、pos[6]をopen_dateとして使用しており、crossover_by_ny_closeメソッド使用前は必ずこのcheck swap以下がadd_swapが０でも実行はされたので、pos[6]が毎回更新されていたが、crossover_by_ny_closeによってpos[6]の値の更新が不規則になり、得られるスワップポイントが検算結果と一致しない
     よってcrossover_by_ny_closeをコメントアウトすると計算が一致
    また、rollover_daysが非常に遅いので、要修正
    →trading_days_set = set(trading_days)が遅い原因

    今は、営業日を最初にintervalに応じてdictを作ってから実行しているが、start_dateとend_dateの期間が長いほど、営業日をinterval=M1で作るのに負荷がかかるから、前のプログラムのようにその都度休日と祝日でないかどうか調べてtimedelta(days=1)を足していく方法の方が計算が早いのではないか。

2024-12-19
    →結局self.business_daysではなくself.each_holidaysとする方が時間短縮になるので変更した
    2019年4月1日以前のスワップの理論値計算で移動平均を計算すると空になるので、修正せよ

2024-12-21
    dtype=datetime64[ns] and Timestampというエラーがkabu_library.pyのget_data_rangeで出現
    →get_data_range内の不等号で比較する値を全てdatetimeにすることで解決

    kabu_swap.pyを2019-4-1以前を含めた時に、2019-3-29におけるすわっぷぽいんとが0になってしまう。
    →おそらく、2019-4-1を境に参照するスワップカレンダーが変わるので、その境目である3-29はスワップポイントが２営業日後を考慮することからうまく計算できていない。つまり3-29の２営業日後は4-2だが、swap_points_dict_theory２は3-31までしかないため、２営業日後が〇になる
    →2019-4-2までself.swap_points_dict_theoryに追加した上で、while文をadd_business_daysに追加することで、start_dateが土日の場合でも正しく営業日がカウントできるようにした

2024-12-22
    cpcvによってデータを分割して最適化をかけ、その結果が過学習しているかどうかをcscvで確かめるコードを書く
    →うまくいかないかもしれないので保留
    ・感度分析やストレステストを実施せよ（最適化されたパラメータを少しずらして結果が大きく変わらないことを確認）
    ・パラメータの複雑さをペナルティとして評価関数に追加（AIC,BIC）
    ・ATR(Average True Range)から理想的なトラップ幅を算出し、その範囲から大幅にズレると結果が著しく悪くなるか確認せよ
    ・スプレッドを実装せよ（トラップ幅が狭いとそれだけ取引頻度が上がるので、スプレッドによるコストがかかるので結果が変わるはず）
    ・Sharp ratioだけでは、ISに対して過敏になるためOOSでの値が極端に低くなってしまう→高次モーメントや他の統計量（ボラティリティ、ヘッジ比率など）も組み合わせよ(All That Glitters Is Not Gold: Comparing Backtest and Out-of-Sample Performance)

2024-12--26
    ダウンロードしたデータのvolumeから一定の割合を設定することで流動性を考慮せよ(All That Glitters Is Not Gold: Comparing Backtest and Out-of-Sample Performance)
    Ziplineを使ってみたい
2024-12-28
    attention(2017)について調べよ
    人間の脳の中で行われていることが、実は行列計算なのでは、という内容の論文を調べよ

2024-12-30
    ・モデル駆動型
        線形回帰
        garch
        ブラウン運動

    ・データ駆動型
    GAN(Quant GANs:Deep Generation of Financial Time Series)やCSDI(CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation)
    でシミュレーションにより、金融時系列データを生成して、それに対して、戦略での最適化をかける

    ・最適化手法を工夫する（cpcv,cscv, sharp ratioにペナルティを加える）

    ・エージェントベースモデルにより異端なトレーダーを再現する

    読みたいけど読めない論文
    Stock Market Simulation by Micro-Macro GAN
    Multi-Agent Market Modeling of Foreign Exchange Rates
    Advances in Agent-Based Computational Finance: Asset Pricing Models from Heterogeneous and Interactive Perspective
    Introduction to Agent-Based Computational Finance
    Ideological Foundation and Research Paradigm in Agent-Based Computational Finance
    Agent-Based Computational Finance and Policy Simulation in Financial Markets: Review and Comment
    Agent-Based Computational Finance on Complex Financial System Perspective: Progress and Prospects
    Learning from others, reacting, and market quality
    Credit risk and lending in an artificial adaptive banking system
    The Behavior of the Exchange Rate in the Genetic Algorithm and Experimental Economies
    Portfolio selection in a multiperiod mean-variance framework(あるかわからない)

    statistical mechanics stock simulationでgoogle scholar検索せよ
    Applications of Statistical Physics in Finance and Economics
    "Agent-Based Computational Economics" by Moshe Levy
    "Agent-based Models of Financial Markets" by Samanidou et al.
    Criticality and market efficiency in a simple realistic model of the stock market
    Model-Agnostic Meta-Learning Techniques: A State-of-The-Art Short Review


2025-1-9
    生成者に入力データとして流動性とスリッページを与えるか
    一般的なabcfで採用されるモデルは需給Dに基づく定義式で市場価格を生成し、流動性とスリッページはあくまで結果に過ぎない
    しかし自分の研究は一般的なabcfと違ってGAN的なアプローチをするため、生成者が各エージェントを騙すように市場価格を生成する必要があるため、とりあえず流動性とスリッページを入力データとして採用

    さらに、生成者への入力データとしての流動性とスリッページは、学習初期で定義式によるそれを採用することで学習を安定させ、学習の途中からは生成者の出力値であるそれに代替することで恣意性をある程度排除する方針。

2025-1-11
    トラリピのスクリプトを参考にすることで、ショートポジションを適切に管理できるように修正したい(https://chatgpt.com/c/6780755a-f2a0-800e-83f4-d0689dd87141)

2025-1-19
    RLAgentクラス内で、tf.convert_to_tensorを使用して初期化してはダメ。tf.Variableを使うべし。
    またtf.Variableで指定した値はassignを使って更新しないといけない
    例) self.positions_index.assign(self.positions_index + 1)

2025-1-23
    gen_lossでdiscriminator_performanceの平均を取っており、これはagent.effective_marginの平均を意味しているが、effective_marginが、生成者の出力値であるcurrent_priceを用いて計算されるためには、利益確定もしくは含み益の更新をしないとダメで、新たにポジションを開く段階ではeffective_marginは更新されない。
    しかし、開いたポジションは例外なく必ず、含み益の更新か利益確定のフェイズを踏むので、effectivemarginがcurrent_priceに依存するはず

    新規注文処理のところで、longの後で、shortしているが、これではlongの方が優先的になってしまって対等性が失われている
    どこで計算グラフの情報が落ちてしまっているか調べるために、新規ポジション、決済、損益更新、それぞれをコメントアウトすることで、探索せよ
    
    →そもそもself.update_assetesそれ自体をコメントアウトして、self.effective_margin.assign_add(current_price)のような簡単なものに置き換えることで、
    動作確認を再度行うべし、

2025-1-28
    effective_marginに生成者と識別者の出力値を直接使用した場合は、もう１つの変数を使うことで(update_effective_margin)計算グラフの情報を保持したまま勾配を計算することに成功した。
    →update_assetes内で、effective_margin以外のすべての計算グラフを保持させたい変数についてもう１つの変数を作り適切に値を更新して勾配が正常に計算できるか確認せよ

2025-1-30
    とりあえず動作するようにはできたが、generationごとにeffective_marginやcurrent_priceが不変の場合があるようなので、不変となる変数がどれかということとその原因を調査せよ
    あとは、lossをupdate_effective_marginを使用していることから、lossの値自体は変化しない
    →一般的にはlossの値が減少することで学習が進むことを人間が確認できるがこの方法が通用しないので、学習の進捗を確認できる方法を調べよ
    あとは、今使用しているのは単純なニューラルネットワークなので、決定論的な推定しかできない。
    →学習が進む＝定常な市場の状態に固定化される
    →市場の価格ではなく、価格の従う分布を学習する確率論的ニューラルネットワークの使用を検討（しかし、これも学習が進む＝市場の従う分布が固定される）

2025-2-1
    有効証拠金と、預託証拠金とポジション損益の総和が不一致のagentが存在する
    →これが丸め誤差なのか、計算アルゴリズムの人為的なミスによるものなのか、確かめよ
    生成者の出力値であるcurrent_priceのヒストグラムと自己相関を逐一調べることで、価格が定常性と非定常性をかわるがわる再現できているかの指標とせよ
    エージェントの収益の分布を調べよ
    各エージェントの相関を各タイムステップごとに計算せよ。
    ペアワイズ相関（各タイムステップごとの相関ではなく全期間の収益の時間平均に対する相関）も計算せよ
    gifを使用したアニメーションでヒートマップがどう変化していくか調べよ

2025-2-2
    完全決済の時にそのポジションは利確されるので、posのadd_required_marginの項目は0になって欲しいが、10^-2 ~ 10^-8のオーダーの値になることがある。
    →おそらくこの誤差と、最後の有効証拠金=預託証拠金+ポジション損益,の不一致が同じ原因と考えられる
    →有効証拠金のオーダーと生成者の出力値のオーダーが違いすぎることで、float32だと丸目誤差が10^-3程度でも起きるため、計算が不一致となる
    →log_scale_factorをtrainable=trueのtf.variableとして定義して、current_priceに乗算することで学習の対象とする
    →それではどうやら学習の対象にはならないので、.gradientで勾配計算するときとoptimizer.apply_gradientsを呼び出す時に明示的にgenerator.model.trainable_variables + [generator.log_scale_factor]とすることで、log_scale_factorを学習の対象としたいのだが、謎のエラーがapply_gradientsに対して出るので対処すべし
    →generatorのモデルの定義の後にself.log_scale_factorをadd_weightで定義して、前述のように.gradientと.apply_gradientsでlog_scale_factorを追加することで学習の対象とすることには成功
    →しかしなぜか、以前にも見られた、全てが0のposがpositionに追加されてしまう現象がまた発生している
    →何もしてないのになぜか解決、依然としてcheck totalと有効証拠金の不一致が見られる

    tensorarrayで保持しているposが計算グラフの情報を保持できているか確認せよ。具体的にはloss=add_required_marginとすることで、current_priceを使用しているadd_required_marginでgradientが計算できるか確認せよ
    →gen_gradient, disc_gradientsの両方で計算ができたことから、add_required_marginは、生成者の出力地であるcurrent_priceと識別者の出力値であるlong_order_size,short_order_size,long_close_position,short_close_positionの計算グラフの情報を保持していると言える

2025-2-4
    2025-1-23のところでも課題として挙げたが、process_new_orderをlongの後にshortを新規注文するように書いてあるがこれではlongの方が買いやすいというバイアスが生まれるのでだめ
    →process_new_orderを買いと売りの同時注文ができるように修正
    updated effective marginのところで、positions.writeとすると例えばpos_id=0であったposがpos_id=1になってpos_id=0のposとして全ての要素が0.0のposが追加されてしまう
    →pos_idをposから消したことでうまくいった。_remove_positionでgatherを使用したことで、pos_idが前倒しにずれることで、posの１つの要素としてpos_idを採用するとその値と、真のpos_idの値が不一致となる。

2025-2-6
    今は需給に関係なく、各エージェントが自分の資産の範囲内で売買したければ自分の意思だけでそれが可能な設定だが、実際には例えば買い注文をしてもそこで誰かが売ってなければ買うことはできない
    order_sizeそのものをlossとして勾配計算したところ、disc_gradientについて、なぜか1th agentだけが非負で、それ以外のagentが0のままだった。
    →order_sizeの更新に、effective_marginの更新にupdate_effective_marginを使用したように、new_order_sizeに一度更新した値を入れてからorder_size=new_order_sizeとする方法と、単純にpython演算子+=を使用する方法を試しましたが、lossをorder_sizeとした場合に依然として1th agentだけがdisc_gradientが非負だった。(seed=42)
    →seed=43に変更すると、1th agentは０となる一方で、2th, 3th agentが非負となった。よって重みの初期値で勾配消失が起きてしまっている。
    →order_sizeそのものをlossに使用して勾配消失が起きてしまっても、実際にはeffective_marginをlossに使用するだけなので問題なし。現在seed値を変えれば全てのagentが非負の勾配を持つことがわかっているのでorder_sizeが計算グラフの情報を保有していることを確認できたので問題解決です。

2025-2-9
    2025-2-2のところにも書いたが、全てが0のposがpositionに追加されてしまう現象が発生。
    →何もしてないのになぜか解決
    ※log_scale_factorを学習の対象にすることには成功したが、学習の方向性の定義が課題。
    2025-2-2にも書いた問題だが、部分決済と完全決済のところで選別基準にsizeと0の大小関係を設定してあるが、fulfilled_size = tf.minimum(close_position, size)という定義でsizeの方が選ばれても、誤差が発生していたとしたら、size -= fulfilled_sizeでsize=0にならない可能性がある。そうすると本来完全決済のはずが、部分決済になってしまい、余計なposがpositionに残る可能性が考えられる。
    →そうではなく、決済処理の後でポジション更新の処理をしているが、決済処理のところで部分決済だとpositionsにそのposは残り、その際にunrealized_profitを一度減算しているにも関わらず、ポジション更新のところで再度そのposに対してunrealized_profitを減算してしまっている


2025-2-11
    2025-2-6にもあるとおり、需給を考慮して各エージェントの発注が約定するかを決定する関数を実装する必要がある
    →未決済注文や未決済ポジションの分配方法としては以下の3つがある
    ・比例分配
    ・ランダム分配
    ・均等分配
    とりあえず比例配分で、match_ordersを実装したが、
    １、今のままでprocess_new_order内でorder_margin < 0となって新規注文が実行されなかった場合にこのままで正常に動作するのか
    ２、process_position_closureにもprocess_new_order内で計算処理しているunfulfilled~orders変数のように、閉じれなかった決済注文をunfulfilledとしておくべきかどうか 
    →2は解決、１もおそらくreturnとなるだけなので問題ないかと思われる。
    ※→ランダム分配と均等分配も追々実装せよ

2025-2-13
    tf.TensorArrayで管理しているpostionsの各要素posを、pos.readとした後に、何もしないと全てが０のposが自動で実装されてしまう誤作動が起きる
    →positions.stack()やpos.writeとすれば解決
    ※unfulfilledなpositionsの計算が正しいかを検算する方法を検討せよ
    ゼロサム性が再現されていない（つまりagent.effective_marginの合計が一定になっていない）
    →確かにポジションの開閉は対となる注文が同数存在して初めて成立するためこの時点ではゼロサム性は保証されるが、各エージェントが自身のタイミングで利確することで、対となるポジションのうち一部が利確によって市場の価格変動に対して不変となり、残りの一部は市場の変動と共に変化するからここでゼロサム性が壊れる
    generationの増加に伴い、計算グラフの情報を持つ変数の数が増えることで、勾配計算が膨大になる
    →tape.reset()を定期的に(10ごとに)入れる
    loss cutが執行された後の挙動が考えられていないため、資産がマイナスのままシミュレーションが進んでしまう。
    →依然としてcheck totalと有効証拠金の不一致が起こるのだが、おそらく、unrealized_profitの減算が重複していることに関連すると思われるが、原因が不明
    →降順で_removed_positionsを呼び出すことで、不足なく全てのposをremoveができ、さらに強制ロスカットの部分で付け足してしまっていたbefore_unrealized_profit = 0を消去することで、強制ロスカットが発動しても計算が一致することを確認。しかし現状では含み損益がある場合のシミュレートができてないのでもしかしたらその場合に不一致が起きるかも

2025-2-14
    tf.shapeを使用すれば、self.positions_indexは不要

2025-2-18
    ・市場の非定常性の再現方法
    (gpt案)
    隠れマルコフモデルで効率的市場と非効率的市場の状態を遷移するモデルを作る（状態A：EMHが成立→リターンがランダムウォーク、状態B：EMHが崩れる→リターンに自己相関やアノマリーが発生
    (日米中株式市場の連動性.pdf)
    非線形共和分モデルの導入

    ・市場の非定常性の評価方法
    (gptの案)
    シャノンエントロピー
    リターンの自己相関
    各エージェントの累積損益をヒストグラム化し、パレート分布をフィッティング
    Hurst指数、ボックスカウント、Detrended Fluctiation Analisys(DFA)　などを使用して市場のフラクタル構造を出力せよ
    単位根検定による非定常性のテスト
    Bai-Perronによる構造変化の検出
    Garchモデルによるボラティリティ変化の評価
    カルバックライブラー距離で、実際の市場のリターン分布と、生成データのリターン分布の距離を測る
    フーリエ変換を局所的に使用して、上記の手法と組み合わせることで、非定常性の評価を補助（または、フーリエ変換で特定の周波数でピークがあるとその生成データは非現実的であるか可能性が高いと評価できる）
    クラスタリング＋inception score(IS)　で、シミュレーションによって生成したデータをトレンド相場やレンジ相場などのように分類して、実際の市場と同じ割合で市場状態を再現できるか
    リターンの歪度と尖度（ファットテールが再現されているか）
    ヒストグラムの変化（確率分布やリターンといったモデルの出力の分布が時間とともにどう変化するか見てみる）
    ヒストリカルボラティリティ（一定期間の価格変動率）を比較
    プロトタイプを発展させて価格変動する生成者が２種類以上ならば、相関行列を計算せよ
    HMM(hidden markov model)やCPD(change point detection)で、市場のレジームシフト（市場が異なるフェーズに移行すること）を検出
    GANの学習進捗の評価方法として潜在空間補間が有効
    リスクリターンのトレードオフ（高リスク資産の期待リターンは、低リスク資産の期待リターンを上回るべき）
    クラスタリング手法（K-means, DBSCAN）を使い、生成データが適切に多様な市場パターンを持つかを確認。
    GAN市場でオプション価格を計算し、ボラティリティ・スマイルを確認することで、GAN市場のリアリティを評価できる

    (市場の効率性と情報消化プロセス.pdf)
    MA-ARCHモデル
    リターンの自己相関
    出来高変動率の自己相関
    ジャンプ後のポラティリティ持続性

2025-2-20
    ・ニューラルネットワークの学習の進捗を損失関数以外で評価する方法
    勾配のノルム
    フィッシャー情報行列で、学習したモデルの汎化性能を評価できるかも
    フィッシャー情報行列の固有値分布を用いることで、保有している情報の豊富さがわかる（https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/67b6b5fe-312c-800e-9331-ba4beee7ab9d）
        (
    Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization
    Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach
    Pathological Spectra of the Fisher Information Metric and Its Variants in Deep Neural Networks
    Training Algorithm Matters for the Performance of Neural Network Potential: A Case Study of Adam and the Kalman Filter Optimizers
    Inverse-Reference Priors for Fisher Regularization of Bayesian Neural Networks
    )

    GANのモード崩壊に関する論文を調べよ

    オプションと先物の違いを理解せよ（https://chatgpt.com/share/67b6c5d8-8e00-800e-b780-38a6d4cacd97）

2025-2-22
    市場の非定常性の評価指標
    (GPTの案: https://chatgpt.com/g/g-bo0FiWLY7-consensus/c/67b6b353-6698-800e-8896-d8bf0fbb7ce3)
        ・効率性の評価指標
        分散比（VR）（「長期的な価格変動の分散」と「短期的な価格変動の分散」）→市場が効率的であるほど、分散比は１に近づく
        スペクトル密度解析（PSD）
        適応的市場仮説（AMH）の評価

        ・市場の非効率性を測る指標
        Hurst指数
        自己相関関数（ACF）
        フラクタル次元
        
        ・市場の効率性と非効率性のバランスを測る
        B = (E_model/E_market)/(I_model/I_market)
        上記で挙げた指標を使って実際の市場の場合と、学習モデルの場合でBを計算することで、効率性と非効率性のバランスを評価
        (Eは効率性の指標で、Iは非効率性の指標)

        ・GANの判別器のエントロピー（H_D）の変動幅(エントロピーボラティリティ)
        σ = √(1/T Σ (H - H_average)**2)
         （エントロピー・ボラティリティ）が高い
        → Discriminator が市場の変化に適応し、柔軟に戦略を変えている（市場の非定常性を学習できている可能性が高い）
            disrcriminatorがうまく学習できておらず適当な（ランダムな）戦略しか出力できない場合もある
            →そのdiscriminatorの総資産が増えていれば、その戦略は有効で、かつ市場も非定常的であると言える
         （エントロピー・ボラティリティ）が低い
        → Discriminator の戦略が固定化されており、市場の変化に対応できていない（非効率な市場を過剰に学習した可能性）
        
        ・市場の効率性とエントロピーの相関
        分散比（VR)、hurst指数とdiscriminatorのエントロピーHの相関
        ρ(H,VR), ρ(H,Hurst)
        相関が強い場合
        市場の効率性が高いときにエントロピーが高くなるなら、Discriminator は効率市場に対して適応できている 可能性が高い。
        相関が弱い場合
        Discriminator が市場の状態を適切に捉えられていない可能性がある。

        ・短期エントロピーvs長期エントロピー
          H_shortとH_longの差が大きい場合
        → 短期的には市場の変化に適応できている（非定常性を学習できている）
        Generator が非効率的な市場を偏って生成している場合、Discriminator は特定の戦略を採用し続けるため、エントロピーは低くなる。これは適応しているのではなく、むしろ 過学習している or バイアスがかかっている 状態かもしれない。
          H_short≒H_longの場合
        → 学習が進んでいない or 市場のダイナミクスを適切に学習できていない可能性
        Generator が市場の非定常性をうまく再現できている場合、Discriminator の戦略は固定されず、エントロピーが大きいまま維持されることがあり得る。
        一方で、Generator が非定常性を再現できていない場合も、Discriminator が適応できず、エントロピーが大きいままになることがある。

        →エントロピーの勾配を計算して、エントロピーボラティリティと共に表示することで、指標の補助的な役割として利用

        ・入力データに摂動を加えてBのロバスト性を調べる

    2025-2-23
        現在generations=100まで試しているが、ほとんど情報グラフをリセットするまで、generatorの生成する価格の推移が単調増加もしくは単調減少になってしまう
        →まずgenerations=1000などとしてこれが永遠に続いてしまいそうか判別する。もし単調性がgenerationsの値が増えていっても消えない場合は、学習の方向性を修正する必要がある（つまり、損失関数がeffective_marginそのものであるのは不適切な可能性がある）
        
        モード崩壊が起きているかどうかも確認せよ
        →そもそも本研究におけるモード崩壊とは(https://chatgpt.com/c/67bbda3c-ac74-800e-bc0c-e4dfb45911b7)
            GANの出力する current_price が特定の値（例: 1.0付近）に固定される
                価格が全く変動せず、市場のダイナミクスが失われている。

            GANの出力する current_price が単調増加または単調減少し続ける
                ランダムな市場変動が存在せず、固定的なトレンドしか学習できていない。

            GANの出力する current_price が完全にランダムになってしまう
                価格変動が「完全なノイズ」となり、市場の特徴を全く持たなくなる（= 学習がうまくいっていない）。
        
        ・current_priceのクラスタリングをすることで、単一の市場ダイナミクスになっていないか確認
        ・current_priceのボラティリティ変化を計算
        ・current_priceの自己相関
        ・current_priceの変化率（リターン）

2025-2-24
    gen_lossを各エージェントの資産の合計と指定しまうとゼロサム性からほとんど変わらなくなってしまうため、傾きもほぼゼロになり学習が停滞する
    →いずれかのエージェントの資産をランダムにgen_lossとすることで、市場の非定常性の再現に挑戦
    →今のところ増え続けてしまっているので、各エージェントが買い注文を増やすことで需給の関係から上昇トレンドを生み出して、それがループしている可能性がある
    →agentの出力値4つの推移を見てみて上記の仮説が正しいか検証せよ
    →generatorの入力地の１つであるsupply_and_demandがmatch_ordersメソッドの前で計算されてしまっていたので、未決済注文の合計で計算するように修正した
    →価格が上昇一辺倒だけでなく、下降もするようにはできた。
    ※→あとは市場の非定常性が再現できるかどうかだけだが、gradienttapeによる計算爆発が起きるため、長期間のシミュレーションが困難になっている
    →今のところは毎回ランダムに選んだエージェントの資産をgen_lossとしているが、学習の方向性がぶれてしまうリスクがある。よってソフトマックスを使用した確率で資産が比較的多いエージェントが選ばれやすいようなlossを使用することも検討
        １「完全ランダム」は学習がブレすぎるので、ある程度の傾向を持たせた方が良い
        ２「ソフトマックス」はバブル方向のバイアスがかかるので、温度パラメータを導入するとバランスが取れる
        ３「中央値に近いエージェント」も一つの選択肢
        ４「最近選ばれたエージェントを除外する」ことでバランスを取る方法も有効
        ５「ボラティリティを考慮する」ことで市場の変動性を維持できる
    →上記の案があるが、完全なランダムであっても資産が増えているエージェントが多ければ、結果としてそのエージェントが選ばれる確率が上がります。つまり、あるダイナミクスに対して資産推移がマジョリティとなるようにlossが選ばれるということです。そしてその様に選ばれたエージェントの資産が減るように、つまりマジョリティが稼ぎにくいような価格を生成する様に学習が進むことになるので、完全にランダムでもいいのでは？

2025-2-27（https://chatgpt.com/c/67bbda3c-ac74-800e-bc0c-e4dfb45911b7）
    ※現在はuse_rule_based=Trueとしているため、人間が定めた恣意的な定義式でliquidityとslippageを計算しているが、
    １、学習後半でuse_rule_based=Falseとすることで、generatorに学習を任せてしまう方法
    ２、lossを、定義式で計算できるliquidityとslippageとgeneratorの出力値の差の項を導入することで、部分的に教師あり学習にして学習を安定させる方法
    がある。いずれにせよ、kの値を定めるgammaの値に人間の恣意性が入り込んでしまうので、どうするか

    ※disc_loss = effective_margin + realized_profit　や、それらに重み付けしたlossの採用を検討
    →異なる重みづけによって多様なエージェントを作ることに繋がり、いかに書いたモード崩壊を避けることにつながるかも

    ※現状effective_marginのみをlossとしているので、agentが全く取引をしない→需給が不変なのでgeneratorの出力値であるcurrent_priceも不変→またagentが取引しない、、、を繰り返すモード崩壊が起きる
    →以下を追加実装せよ
        異なる時間軸のエージェントを導入する（短期・長期トレーダー）
        需給による価格変動をリアルに再現する（流動性モデルの強化）
        外部ショックやストップロスの影響を入れる（クラッシュの再現）→ストップロスは実装済みで、外部ショックはgeneratorのlossをランダムに選ぶ仕様にすることで、結果としてgeneratorの学習の方向性が変わるので外部ショックを再現できるのではないか
        マーケットメーカーの存在を考慮する（市場の安定性を再現）
        投機的トレーダーを追加する（ボラティリティの変動性を再現）

    →外部ショックの再現と、市場の非効率的なダイナミクスへの固定化の問題はgen_lossとしてランダムにエージェントの資産を選択する方法で解決し得るが、リスクとしていいかが挙げられる
        完全なランダム選択ではやはり特定のダイナミクスしか再現できなくなるモード崩壊の問題は起き得る

2025-3-2(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/67c41cfa-2da4-800e-a63b-a48dc83680cd)
    ロスカットで離脱したエージェントの情報を一部引き継いで参戦できるエージェントの実装
    SHAP（SHapley Additive exPlanations）によってAIの予測を説明できるようにする
    discriminatorに関しては単なるネットワークだと固定された分布を学習するにとどまるので、強化学習エージェントにすることで、市場の変化に対応できる
    DQN
    PPO（Proximal Policy Optimization）
    AlphaStar
        自己対戦であるエージェントが他のエージェントを出し抜く戦略を学び市場の非定常性をよりリアルに再現できる

    マクロ経済指標（金利、インフレ率、経済成長率）をgeneratorの入力とする案

    エージェントの入力値にボラティリティなどのインジケータを追加してよし

    外部ノイズの実装（重みパラメータの初期値はその更新にランダム性が含まれるからそれを外部ノイズと解釈すればいいかもしれないが、generatorが特定の分布で固定され、それに対してagentも特定の戦略しかとらなくなった場合、そのランダム性のスケールが出力値に比べて極端に小さい場合、人為的に外部ノイズを挿入する必要があるかも）


2025-4-22
    以下のようにpytorchを使用すれば重みに依存した出力値が次のステップの入力値になるようなオンライン学習がこのmacでもできるかも
    https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/680740cd-85b0-800e-80e5-60254704ddbd

2025-4-29
    まずは入力データとなる値（需給やcurrent_price)などをPCAなどの時限削減によって重みに対する依存度を減らすべし
    current_priceなどモデルの重みに依存しているパラメータの勾配ベクトルに対してPCAなどを実行、その上で値そのものは変わらないように以下のようにする(https://chatgpt.com/c/681093d1-f654-800e-b268-35aeda3cdfe8)
    current_price_fixed = tf.stop_gradient(current_price) + 0.0 * tf.reduce_sum(compressed_dep)

2025-5-9
    以下の論文では非定常的なデータをクラスタリングによって暗黙的にラベル付けしてVAEによる潜在変数zを獲得関数に使用する手法。
    Context-Based Meta-Reinforcement Learning with Bayesian Nonparametric Models(https://chatgpt.com/c/681c4101-ce20-800e-85f3-6eecea033e55)

    以下の論文で過去の勾配を確率的にサンプリングしておいて、現在の勾配との内積を取ることで過去の情報を考慮しつつ学習を進めることのできるメタオプティマイザの提案
    https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c3b6f-8524-800e-9a9c-d5d172cf4a16

    以下の論文で、ドメインシフトを伴う不均衡なタスク系列においてメタ学習を行う新たな手法を提案
    Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c3a35-2bcc-800e-9f1c-648343120ade)

    Dynamic Sparse Training（DST）(https://chatgpt.com/g/g-bo0FiWLY7-consensus/c/681c3791-4eb8-800e-a299-b08492425ce1)
        以下の論文ではBiDSTを提案し、重みと共にマスクをも学習
        Advancing Dynamic Sparse Training by Exploring Optimization Opportunities(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c383c-5704-800e-9684-0568bafeb862)

    以下の論文ではrPropを採用し、勾配の大きさではなく符号のみで学習を進める
    Analysis Resilient Algorithm on Artificial Neural Network Backpropagation(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c345c-dfb0-800e-aec9-8aaf621e2146)

    以下の論文ではmePropを採用し、出力ベクトルの勾配の上位k個の要素のみを残し、それ以外を0にすることで、勾配計算をスパース化
    meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c352c-d394-800e-862e-89f38147ef19)

    以下の論文では「ディザード・バックプロップ（dithered backprop）」という方法を提案。これは、非減算型ディザ（NSD）による確率的量子化を中間勾配に適用することでスパース性と低精度の非ゼロ値を同時に導入し、計算効率を向上させる
    DITHERED BACKPROP: A SPARSE AND QUANTIZED BACKPROPAGATION ALGORITHM FOR MORE EFFICIENT DEEP NEURAL NETWORK TRAINING(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c319c-1fb4-800e-95f0-b7031efeaa24)

    そもそも誤差逆伝播以外の方法で最適化をかけることも有効
    1,ブロック座標降下（BCD）法 A Proximal Block Coordinate Descent Algorithm for Deep Neural Network Training(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c2d8e-1504-800e-8cd7-7b637d0671cf)

    2,ADMM (交互方向乗数法)

    3,進化的アルゴリズム

    4, 勾配推定に基づくメソッド
        勾配をアンサンブル平均によって近似的に計算。これにより微分不可能であってもブラックボックスであっても学習可能

    5, Hebbian学習
        教師なし学習の一種。入力と出力の共起・相関を検出してネットワーク構造を変化させる

    以下の論文ではSparse propを提案し、非構造的スパース性でも高精度で学習可能
    SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c2548-a580-800e-9524-5cae8328c9a7)

    以下の論文ではDecorrelated Backpropagation (DBP) を提案し、入力データを相関が最小になるような基底に変換
    Efficient Deep Learning with Decorrelated Backpropagation(https://chatgpt.com/c/681c2977-4db0-800e-a306-436b12773ce9)

    以下の論文でネスト化ワッサースタイン距離を提案し、それを獲得関数とする強化学習によって高報酬の過去データを再利用しつつ、意味的な分布一致を達成する
    Nested-Wasserstein Self-Imitation Learning for Sequence Generation(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681c3c63-e444-800e-a6e1-ec5cd9aee6e4)

    以下の論文は重みの情報をメモリに保存しないで、逆伝播の際に再計算することで、メモリ爆発を回避する手法。順伝播と逆伝播の間には重みの更新があるため、逆伝播の際に再計算をするだけでは値がずれてしまうので、将来の重みに近い値を予測することでズレを解消
    Decoupled neural network training with re-computation and weight prediction(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681d4e69-746c-800e-b815-fcfd6cb27d08)

    以下の論文は複数DNNを重要な重みを共有することで、効率よく学習させる手法を提案。重みの重要度はfisher情報で評価
    Fast and Scalable In-memory Deep Multitask Learning via Neural Weight Virtualization(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681d50a5-b970-800e-b2ae-bdcb4eac2255)
    Weight Separation for Memory-Efficient and Accurate Deep Multitask Learning(https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/681d5793-3cc4-800e-8d43-28ec7b5fa585)
        →hessianは2回微分のためO(n^2)だが、正則条件下では一回微分の二乗平均(=fisher情報)と等しいので、正則条件が満たされると仮定して近似的に計算を軽くできる
        cf) EWC（Elastic Weight Consolidation） 「重要な重みを保護しつつ、新しいタスクの学習を許容する」ことで、「破滅的忘却（Catastrophic Forgetting）」を防ぐ方法。

    2025-6-1
        lambda濫用を防ぐことでメモリ爆発を最小限に抑えることに成功し、generation=75くらいまではkillされずに進めることができた。
    
        bypass_node_impactを追加実行するとエラーが出るのでその対処をせよ
        __getitem__をsplit_vectorのかわり使用したが、これで計算がずれてないか確認せよ
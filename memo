kabu_montecarlo.pyが終わったら、kabu_backtest.pyを、cumulativeなreturnをtxtfileに保存するように修正して、再度kabu_montecarlo.pyを実行せよ
svmとGAの論文を読んで、完全に再現
svmとGAの論文で使われている、動的なGAについて、ハイパーパラメータをどう決めるか、その動的な手法が高精度なのはどのような原因が考えられるか解読する
svmとGAの論文以外にcitationの多い、overfitしていない先行研究を探す→Modeling, forecasting and trading the EUR exchange rates with hybrid rolling genetic algorithms-Support vector regression forecast combinations（citation132）
kabu_montecarlo.pyはパラメータを全て一様分布から生成しているが、これも先行研究を探す
covariance penalitesの論文にある、CV以外の先行研究となった手法を調べる
cpcvを日足より短い”大量の”データを使って再度試す
うまくいきそうなら、その結果に対してcscvでpboを確認
transaction costsを導入すべきか考える、また最適化する関数の返り値がeffective_marginでは、過学習するからリスクを考慮した指標を使用するべきで、sharp_ratioか、最大ドローダウンなどを組み合わせた独自の関数を作るか考える→遺伝プログラミングで評価関数を自動で生成すれば良いのではないか


・各戦略が生成したリターンの統計的な性質に焦点を当てた論文

A Bayesian Approach to Measurement of Backtest Overfitting (2021年のものなのでまだcitationが少ない)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66e2115f-c26c-800e-ac38-9ebc1d24fa2e
指定した平均と標準偏差から複数の投資戦略のリターンを生成し、MCMCを使って戦略の平均と共分散をサンプリング。その後、その平均と共分散からout_sampleリターンを生成し、シャープレシオを計算。




・各戦略のパラメータを具体的に最適化することに焦点を当てた論文
    <GA関連>
Parameter Optimization for Trading Algorithms of Technical Agents
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66e214f0-9a78-800e-a1cc-f45886fd52c0
BOとGAを使って累積リターンが最大となるようなRSIの下限値と上限値、ボリンジャーバンドの平均と標準偏差を推定
ロールフォワード法によるoverfit軽減

FOREX Trading Strategy Optimization (citationほぼ0)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66dc4040-d17c-800e-aada-7f882fe2e79c
GAを使って移動平均の次数、取引頻度のパラメータ、最小閾値を最適化
trainとtestを分けただけ



    <svm関連>
Combining Support Vector Machine with Genetic Algorithms to optimize investments in Forex markets with high leverage (citation77)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66de16a3-db5c-800e-8c3a-d8228ae1bd8b
svmにより、市場のトレンドを３つに判別し、各トレンドに対応するGAで別々に最適化、GAはハイパーミューテーション、ハイパーセレクションを実行
svmに対してkfold交差検証を実施、GAに対してはtrainとtestを分けただけ

Modeling, forecasting and trading the EUR exchange rates with hybrid rolling genetic algorithms-Support vector regression forecast combinations (citation132)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66df6ac1-189c-800e-ba55-fd18a8a7e128
svrによって過去の為替レートから１日先の予測値を算出、GAでsvrのハイパーパラメータを調整。svrの入力データは教師データ：ARMAモデルやNNによって得られる１日先の予測値、教師ラベル：実際の１日先の値、として学習。GAの評価関数は、年率リターン-10*RMSE-0.001*(サポートベクターの数/トレーニングサンプル数)
svrに含まれる正則化項Cによって過学習を抑えられる
※為替データをそのまま与えるとoverfitしてしまうが、例えば移動平均線の値を与えると、入力データが平均化されていることで、overfitしにくくなるのではないか



<その他>
Evaluating machine learning classification for financial trading: An empirical approach (citation200)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66e0bb4c-2f04-800e-981e-684f82061d9e
シンプルな機械学習モデル（OneR, C4.5, JRip, Logistic Model Tree, KStar, Naïve Bayes）を使用し、予測精度が50%程度だったが高い収益性を誇った。異なる市場でも安定した結果となった。シンプルなモデルのため計算コストが低いことを活かして定期的に再訓練ができる

Data selection to avoid overfitting for foreign exchange intraday trading with machine learning
https://www.perplexity.ai/search/data-selection-to-avoid-overfi-NQKC0.7bQuqe17n3LDPRdA
新しくパスロス指標(精度＊log(正規化されたinsampleリターン)/収益率)を提案、２つの学習目的（回帰と分類）、２つの取引戦略（１機関保有、切り替えまで保有）、４つの機械学習モデルを使用（NN、ランダムフォレスト、SVM/SVR、XGBoost）


・新しい手法
Avoiding Backtesting Overfitting by Covariance-Penalties: an empirical investigation of the ordinary and total least squares cases (citation12)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66da70eb-4eb0-800e-a45b-4825778a7845
共分散ペナルティ補正法によるoverfitの回避




・交差検証の応用
The Probability of Backtest Overfitting (citation132)
https://chatgpt.com/g/g-hxDOCBQrs-paper-interpreter-japanese/c/66dba0a1-bcec-800e-9da5-89196b3a70b5
cscv法によるPBOの計算によりoverfitを定量的に評価できる

effective_margin+{(sharp_ratio-1)-effective_margin*max_draw_down}*effective_margin


・kabu_backtestで通貨ペアの二カ国のいずれかの祝日に被っている日は決済できないので、それをposition closure processingに追加で実装せよ
また、二カ国両方とも祝日の場合はどうなるか確認せよ
→swap pointの受け渡しができないだけで、祝日はfxの取引は通常通り行えるので、修正不要

・kabu_swap.pyを、minkabuだけでなくoandaからもスワップポイントをスクレイピングできるようにしつつ、oandaのサイトには掲載がない2019年4月より前のデータに対しては比率を計算して、理論値を算出するように実装せよ。

kabu_compare_intrestrate_and_oandascraping.py内で、calculate_swap_averages関数を呼び出す際に指定されるcurrent_startとcurrent_endの範囲で、kabu_oanda_swapscraping.pyで生成されるcsvファイルがあるかを判定してしまってる。
範囲を修正せよ。→解決

その後、average_buy_swap、average_sell_swapがそれぞれ０と２になってしまう原因を追求せよ
→解決

2024-11-03
kabu_compare_intrestrate_and_oandascraping.pyでほぼ正しい結果が出たが、2021-12~2022-02にかけての平均スワップの値が飛んでしまっているので、計算エラーかを確認せよ。


2024-11-09
kabu_swap.pyの__init__内のif found_file以下が正常に動作するかをkabu_backtest.pyを走らせて確かめる(つまりwebsite="oanda",interval="M1",link=link)
→確認完了

kabu_backtest.pyのlong_onlyのcheck swap　以下を修正したので、動作確認をする（つまり、website="minkabu",interval="1d"）
→long_onlyのcheck swap以下にpos[8] += ...を追加し、check_min_max_effective_marginを追加したことで変わるのは、シャープレシオと最大ドローダウンだけなので、check totalの値と有効証拠金の値の一致不一致には関係ない
動作確認は完了

website="oanda"でlong_onlyの時、計算が合わないようなので、確認（つまりまずはwebsite="minkabu",strategy="long_only"で動作確認）
→website="minkabu"では合うことを確認、website="oanda"では計算が合わないことを確認

→おそらく原因は、スクレイピングで得たデータは日本の日付でのスワップデータなので、それを世界基準に直さないとスワップポイントの計算でずれが生まれるのではないか


oanda証券のスワップカレンダーに記載されているスワップポイントの単位を調べよ

2024-11-10
kabu_swap.pyの以下の部分を追加で実装せよ

        # データ取得の制限を確認
        if start_date < datetime(2019, 4, 1):
            print("2019年4月以前のデータはありません。")
            start_date = datetime(2019, 4, 1)

2024-11-12
2024-11-09での確認結果と異なり、websiteやpairの値に関わらず計算が合わない場合があることを確認
Nov 2, 9のコミットでは、Nov 10とは異なり、計算が一致
Nov 9、でEURGBP website=oandaの時のみ計算の不一致を確認（初期条件はinterval=1d,initial_funds=2000000, grid_start=0.84, grid_end=0.94, ordersize=3000, num_traps_options = 100, profit_width=100）この条件では強制ロスカットが作動するが、website=oandaの時のみ計算が合わない
→kabu_swap.pyのget_total_swap_pointsのif self.website==oanda以下のwhile current <= current_dateの部分で、イコールが不要でさらにopen_dateからcurrent_dateまでの営業日数分だけスワップポイントを取得して足し合わせる必要がある。よってwhile current < open_date+timedelta(days=rollover_days) とするのが適切。この修正により、初期条件は(start_date=2021-01-04 end_date=2021-04-01,interval=1d,initial_funds=100000, grid_start=0.86, grid_end=0.91, ordersize=3000, num_traps_options = 100, profit_width=100) この初期条件で強制ロスカットが作動するが計算が一致することを確認
→while current < open_date+timedelta(days=rollover_days)　ではスワップポイントが０になってしまうので不適

Nov 9の時点で、
    website=oandaでかつM1のデータの場合に、ロスカットする場合もそうでない場合も計算が一致することを確認せよ→次に記す初期条件で強制ロスカット時も計算が合うことを確認(pair="EURGBP=X" interval=M1 website=oanda start_date=2021-01-04 end_date=2021-02-01 initial_funds=100000 grid_start=0.86 grid_end=0.91 strategies=long_only order_sizes=10000 num_traps_options=100 profit_width=100)
    →website=oandaでかつM1のデータの場合に、ロスカットする場合は計算が一致することを確認（interval=1dでも確認）
    kabu_swap.pyのif self.website == oanda以下の、返り値の部分で、oanda証券のサイトにはロールオーバーを考慮したスワップポイントが記載されているため、サイトにデータのある2019/04以降のデータでは、返り値はこの値で良いが、2019/04以前のデータはないため、自分でスワップポイントを計算する際はrollover_daysをかけないとだめ
    get_data_rangeメソッドで、dateとcurrent_startが完全に一致しないとstart_collectingフラグがTrueにならないので、たとえばEURGBPの2021/01/01のデータはないので、current_startをその日にするとダメ、修正せよ


2024-11-14
get_holidays_from_pairメソッド内でholidays.CountryHolidaysで指定した国の祝日をdict型に収納し、__init__以下でself.each_holidaysに保有させているが、祝日が日本語になってしまうため、multiprocessingによるpickle化ができない。よって、CountryHoliday().keys()のみをholidays_dictに保有させることで、祝日の名前を除外して日付のみを保有でき問題が解決する

2024-11-16
改めてkabu_swap.pyのget_total_swap_points内のwhile文を修正し、interval="1d"の時はwebsite="oanda"で計算が一致することを確認
→interval="M1"でロスカットが起こる場合(pair="EURGBP=X" interval=M1 website=oanda start_date=2021-01-04 end_date=2021-04-01 initial_funds=100000 grid_start=0.86 grid_end=0.91 strategies=long_only order_sizes=10000 num_traps_options=100 profit_width=100)に計算が一致するか確認中
 またロスカットが起きない場合も確認(pair="EURGBP=X" interval=M1 website=oanda start_date=2021-01-04 end_date=2021-02-01 initial_funds=10000000 grid_start=0.86 grid_end=0.91 strategies=long_only order_sizes=1000 num_traps_options=100 profit_width=100)

 website="oanda"の時は日付が日本になっているので、現地時間に直す必要がある

 2024-11-17
 kabu_backtest.pyで次の初期条件で計算の不一致を確認(pair: "AUDNZD=X", interval: 1d, website:minkabu, start_date:2019-11-01 00:00:00, end_date:2019-11-30 00:00:00, initial_funds:100000000, grid_start:1.02, grid_end:1.14, strategies:['long_only'], entry_intervals:[0], total_thresholds:[10000], order_sizes:[1000],num_trap_options:[100], profit_widths:[0.01], densities:[10])

 2024-11-19
 kabu_backtest.pyで上記の初期条件でinitial_fund=100000にすると計算エラーが発生することを確認
 →check swapをコメントアウトすると計算が一致することからスワップポイントの計算が原因であることがわかった
 →結局は、swapではなく、なぜかlong_onlyの時だけfor i in range(len(data))の次の、if margin_maintenance_flagの部分が、if margin_maintenance_flag or order_capacity_flag:になっていた。
  これだと、order_marginが０以下になったときに、ここでbreakしてしまうことで為替の変動に伴って発生する、すでに保有しているポジションの価格変動やスワップポイントの変動もしないままになるので、結果が合わなくなる

以上を踏まえて以前試した、計算結果が一致しない条件で再度確認(EURGBP=X, interval: M1, website:oanda, start_date:2021-01-04 00:00:00, end_date:2021-04-01 00:00:00, initial_funds:100000, grid_start:0.86, grid_end:0.91, strategies:['long_only'], entry_intervals:[0], total_thresholds:[1000], order_sizes:[10000],num_trap_options:[100], profit_widths:[100], densities:[10])

2024-11-21
上記の条件で確認したところ、不一致が確認された。なお2021-01-04 17:55:00 でロスカットされる。
試しに、check swapの部分をコメントアウトし、最後のswap_valueの計算を0としたところ、ロスカットが実行されていても一致
→ロスカットが実行された場合の、最後のswap_valueの計算が間違っているのではないか。

あとは、for i in range(len(data))とあり、dataの量が非常に多いと計算が重くなってしまう
→バッチ処理はどうか

そもそも、2024-11-19の最後に記した初期条件では、2021-01-04から始まり、2021-01-04 17:55:00でロスカットが執行されるが、website==oandaの時には、rollover_daysが0であるにも関わらず、スワップポイントが付与されてしまう仕様になっていたので、
if rollover_days== 0 と if rollover_days >= 1を追加して、0の時はそもそも計算などせずに0を返すようにkabu_swap.pyを修正した
しかし、その修正前に起きた計算の不一致の本質的な解決にはおそらくなっていない。つまり、特定の条件下でのみ計算の不一致が生じる問題の解決にはなっていない。
→とりあえず、修正したkabu_swap.pyで2024-11-19の最期に記した初期条件で、計算が一致するか確認

2024-11-23
kabu_swap.pyで、ロールオーバーの計算はadd_business_daysメソッドにより、ニューヨーク時間に直してから計算してるが、get_total_swap_pointsでwebsite=oandaの時に、self.swap_points_dictからスワップポイントのデータを取得する際に元となっているoanda証券のサイトでは、日付が日本になっているため、齟齬が生じる

2024-11-28
kabu_swap.pyをtest_get_holidays.pyに基づいて大幅に修正したが、正常に動作しないのでデバッグせよ

2024-11-30
kabu_swap.pyで、self.business_holidaysを作る際に、祝日を１年単位で取得した後に、それに含まれない時間帯をbusiness_daysとしているが、rolloverを計算する際に２営業日後の情報が必要
なので、12/31など年末ギリギリのデータが来たときに、その２営業日後が正しく計算できない可能性があるので、修正せよ

2024-12-1
ロスカットが実行される場合にswap_valueが正しく計算されない模様。値が0になってしまうので、おそらくswap_value計算時のget_total_swap_pointsメソッド周りでエラーがあると思われる

2024-12-3
    kabu_swap.pyのget_total_swap_pointsメソッドで、ロールオーバーを計算する前に、open_dateとcurrent_dateを、pairで指定される二カ国の現地時間でのNYクローズと比較することで、それを跨いでいない場合はそもそもロールオーバーは０に決まっているので計算する必要がない、という条件分岐を追加せよ
    →crossover_ny_closeメソッドをSwapCalculatorに追加した。
    https://www.oanda.jp/course/ty3/swap が見れなくなっているので、ny4に移行したほうがいいかも。さらに２つの違いの詳細を把握せよ
    →oanda証券のミスだったので、現在は復旧したためとりあえず問題なし。
    →pairの右側の単位でスワップポイントが表示されている(例えば、USDJPYならば円)ので、単位を合わせよ。

2024-12-5
    2019-4-1以前ではスワップカレンダーがないので、ScrapeFromOanda内で、計算するメソッドを定義し、その関数をSwapCalculatorのinitのところでwebsite=="theory"で動作するようにもせよ


2024-12-7
    kabu_compare_bis_intrestrate_and_oandascraping.py で累積平均や、移動平均の部分でもtheoryを普通の平均で計算してしまっているので、修正
    →このままで問題なし

2024-12-10
    scrape_from_oandaの手前で、found_fileの名称を決めているが、例えばend_date = 2024-10-31の時に、file名が2024-10-30のようにその1日前であっても、最初からscrapingしてしまうので、近い日付があればscrapeするのはその続きからになるように修正せよ
    また、kabu_oanda_swapscrapingのscrape_from_oandaと、kabu_swap.py内のScrapeFromOanda内のscrape_from_oandaメソッドを統一した方が良いのでは？

2024-12-12
    kabu_library.py のget_swap_points_dictメソッドのmissing_rangesに2019-4-1以前の範囲が指定された場合、その前段階ですでにファイルにある範囲のデータをダウンロードしているにも関わらず、scrape_from_oanda関数内でstart_date=2019-4-1,end_date=datetime.now(jst)としてしまうので、またこの範囲でスクレイピングが実行されてしまう。
    →get_swap_points_dictの最初で2019-4-1以前であれば、start_date=2019-4-1,end_date=datetime.now()とすることで、あらかじめ理論値計算に必要な、実際の値を可能な限りスクレイピングすることで解決

2024-12-14
    kabu_swap.pyで2019-4-1以前の場合にswap_points_dictを獲得すると、2019-3-30が土曜日にも関わらず、スワップポイントの値が０でない状態になる。
    おそらくkabu_compare_bis_intrestrate_and_oandascraping.pyのcalculate_theory_swap内の辞書output_dataを作成する際に時差の関係で、日付がずれてしまい、本来金曜日の値のはずが土曜日30日のスワップポイントとして表示されていると思われる
    →kabu_swap.pyのrollover_daysの計算を修正することで、平日のみを考慮して解決
    しかし、その部分の計算が非常に遅いので、要修正

    また、kabu_library.pyのget_data_range関数が、current_startやcurrent_endに休日を指定してしまうと、データがないため適切に範囲指定できない
    →current_start,current_endに最も近い日付にずらすようなアルゴリズムを追加実装せよ

2024-12-15
    前日の課題２つ（rollover_daysの計算、get_data_range)が未解決
    →1つ目はkabu_swap.pyを過去に戻って逐一営業日判定する方法の方が早いかも。比較せよ。

2024-12-17
    結局rollover_daysをwebsite ==oandaの時は計算しないことで、大幅な時間短縮には成功した。
    しかし、計算の不一致が発生し、kabu_swap.pyを遡っても原因不明
    →check swap 以下でget_total_swap_pointsでスワップポイントを計算する際に、pos[6]をopen_dateとして使用しており、crossover_by_ny_closeメソッド使用前は必ずこのcheck swap以下がadd_swapが０でも実行はされたので、pos[6]が毎回更新されていたが、crossover_by_ny_closeによってpos[6]の値の更新が不規則になり、得られるスワップポイントが検算結果と一致しない
     よってcrossover_by_ny_closeをコメントアウトすると計算が一致
    また、rollover_daysが非常に遅いので、要修正
    →trading_days_set = set(trading_days)が遅い原因

    今は、営業日を最初にintervalに応じてdictを作ってから実行しているが、start_dateとend_dateの期間が長いほど、営業日をinterval=M1で作るのに負荷がかかるから、前のプログラムのようにその都度休日と祝日でないかどうか調べてtimedelta(days=1)を足していく方法の方が計算が早いのではないか。

2024-12-19
    →結局self.business_daysではなくself.each_holidaysとする方が時間短縮になるので変更した
    2019年4月1日以前のスワップの理論値計算で移動平均を計算すると空になるので、修正せよ

2024-12-21
    dtype=datetime64[ns] and Timestampというエラーがkabu_library.pyのget_data_rangeで出現
    →get_data_range内の不等号で比較する値を全てdatetimeにすることで解決

    kabu_swap.pyを2019-4-1以前を含めた時に、2019-3-29におけるすわっぷぽいんとが0になってしまう。
    →おそらく、2019-4-1を境に参照するスワップカレンダーが変わるので、その境目である3-29はスワップポイントが２営業日後を考慮することからうまく計算できていない。つまり3-29の２営業日後は4-2だが、swap_points_dict_theory２は3-31までしかないため、２営業日後が〇になる
    →2019-4-2までself.swap_points_dict_theoryに追加した上で、while文をadd_business_daysに追加することで、start_dateが土日の場合でも正しく営業日がカウントできるようにした

2024-12-22
    cpcvによってデータを分割して最適化をかけ、その結果が過学習しているかどうかをcscvで確かめるコードを書く
    →うまくいかないかもしれないので保留
    ・感度分析やストレステストを実施せよ（最適化されたパラメータを少しずらして結果が大きく変わらないことを確認）
    ・パラメータの複雑さをペナルティとして評価関数に追加（AIC,BIC）
    ・ATR(Average True Range)から理想的なトラップ幅を算出し、その範囲から大幅にズレると結果が著しく悪くなるか確認せよ
    ・スプレッドを実装せよ（トラップ幅が狭いとそれだけ取引頻度が上がるので、スプレッドによるコストがかかるので結果が変わるはず）
    ・Sharp ratioだけでは、ISに対して過敏になるためOOSでの値が極端に低くなってしまう→高次モーメントや他の統計量（ボラティリティ、ヘッジ比率など）も組み合わせよ(All That Glitters Is Not Gold: Comparing Backtest and Out-of-Sample Performance)

2024-12--26
    ダウンロードしたデータのvolumeから一定の割合を設定することで流動性を考慮せよ(All That Glitters Is Not Gold: Comparing Backtest and Out-of-Sample Performance)
    Ziplineを使ってみたい
2024-12-28
    attention(2017)について調べよ
    人間の脳の中で行われていることが、実は行列計算なのでは、という内容の論文を調べよ

2024-12-30
    ・モデル駆動型
        線形回帰
        garch
        ブラウン運動

    ・データ駆動型
    GAN(Quant GANs:Deep Generation of Financial Time Series)やCSDI(CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation)
    でシミュレーションにより、金融時系列データを生成して、それに対して、戦略での最適化をかける

    ・最適化手法を工夫する（cpcv,cscv, sharp ratioにペナルティを加える）

    ・エージェントベースモデルにより異端なトレーダーを再現する

    読みたいけど読めない論文
    Stock Market Simulation by Micro-Macro GAN
    Multi-Agent Market Modeling of Foreign Exchange Rates
    Advances in Agent-Based Computational Finance: Asset Pricing Models from Heterogeneous and Interactive Perspective
    Introduction to Agent-Based Computational Finance
    Ideological Foundation and Research Paradigm in Agent-Based Computational Finance
    Agent-Based Computational Finance and Policy Simulation in Financial Markets: Review and Comment
    Agent-Based Computational Finance on Complex Financial System Perspective: Progress and Prospects
    Learning from others, reacting, and market quality
    Credit risk and lending in an artificial adaptive banking system
    The Behavior of the Exchange Rate in the Genetic Algorithm and Experimental Economies
    Portfolio selection in a multiperiod mean-variance framework(あるかわからない)

    statistical mechanics stock simulationでgoogle scholar検索せよ
    Applications of Statistical Physics in Finance and Economics
    "Agent-Based Computational Economics" by Moshe Levy
    "Agent-based Models of Financial Markets" by Samanidou et al.
    Criticality and market efficiency in a simple realistic model of the stock market
    Model-Agnostic Meta-Learning Techniques: A State-of-The-Art Short Review


2025-1-9
    生成者に入力データとして流動性とスリッページを与えるか
    一般的なabcfで採用されるモデルは需給Dに基づく定義式で市場価格を生成し、流動性とスリッページはあくまで結果に過ぎない
    しかし自分の研究は一般的なabcfと違ってGAN的なアプローチをするため、生成者が各エージェントを騙すように市場価格を生成する必要があるため、とりあえず流動性とスリッページを入力データとして採用

    さらに、生成者への入力データとしての流動性とスリッページは、学習初期で定義式によるそれを採用することで学習を安定させ、学習の途中からは生成者の出力値であるそれに代替することで恣意性をある程度排除する方針。

2025-1-11
    トラリピのスクリプトを参考にすることで、ショートポジションを適切に管理できるように修正したい(https://chatgpt.com/c/6780755a-f2a0-800e-83f4-d0689dd87141)

2025-1-19
    RLAgentクラス内で、tf.convert_to_tensorを使用して初期化してはダメ。tf.Variableを使うべし。
    またtf.Variableで指定した値はassignを使って更新しないといけない
    例) self.positions_index.assign(self.positions_index + 1)

2025-1-23
    gen_lossでdiscriminator_performanceの平均を取っており、これはagent.effective_marginの平均を意味しているが、effective_marginが、生成者の出力値であるcurrent_priceを用いて計算されるためには、利益確定もしくは含み益の更新をしないとダメで、新たにポジションを開く段階ではeffective_marginは更新されない。
    しかし、開いたポジションは例外なく必ず、含み益の更新か利益確定のフェイズを踏むので、effectivemarginがcurrent_priceに依存するはず

    新規注文処理のところで、longの後で、shortしているが、これではlongの方が優先的になってしまって対等性が失われている
    どこで計算グラフの情報が落ちてしまっているか調べるために、新規ポジション、決済、損益更新、それぞれをコメントアウトすることで、探索せよ
    
    →そもそもself.update_assetesそれ自体をコメントアウトして、self.effective_margin.assign_add(current_price)のような簡単なものに置き換えることで、
    動作確認を再度行うべし、

2025-1-28
    effective_marginに生成者と識別者の出力値を直接使用した場合は、もう１つの変数を使うことで(update_effective_margin)計算グラフの情報を保持したまま勾配を計算することに成功した。
    →update_assetes内で、effective_margin以外のすべての計算グラフを保持させたい変数についてもう１つの変数を作り適切に値を更新して勾配が正常に計算できるか確認せよ

2025-1-30
    とりあえず動作するようにはできたが、generationごとにeffective_marginやcurrent_priceが不変の場合があるようなので、不変となる変数がどれかということとその原因を調査せよ
    あとは、lossをupdate_effective_marginを使用していることから、lossの値自体は変化しない
    →一般的にはlossの値が減少することで学習が進むことを人間が確認できるがこの方法が通用しないので、学習の進捗を確認できる方法を調べよ
    あとは、今使用しているのは単純なニューラルネットワークなので、決定論的な推定しかできない。
    →学習が進む＝定常な市場の状態に固定化される
    →市場の価格ではなく、価格の従う分布を学習する確率論的ニューラルネットワークの使用を検討（しかし、これも学習が進む＝市場の従う分布が固定される）

2025-2-1
    有効証拠金と、預託証拠金とポジション損益の総和が不一致のagentが存在する
    →これが丸め誤差なのか、計算アルゴリズムの人為的なミスによるものなのか、確かめよ
    生成者の出力値であるcurrent_priceのヒストグラムと自己相関を逐一調べることで、価格が定常性と非定常性をかわるがわる再現できているかの指標とせよ
    エージェントの収益の分布を調べよ
    各エージェントの相関を各タイムステップごとに計算せよ。
    ペアワイズ相関（各タイムステップごとの相関ではなく全期間の収益の時間平均に対する相関）も計算せよ
    gifを使用したアニメーションでヒートマップがどう変化していくか調べよ

2025-2-2
    ※完全決済の時にそのポジションは利確されるので、posのadd_required_marginの項目は0になって欲しいが、10^-2 ~ 10^-8のオーダーの値になることがある。
    →おそらくこの誤差と、最後の有効証拠金=預託証拠金+ポジション損益,の不一致が同じ原因と考えられる
    →有効証拠金のオーダーと生成者の出力値のオーダーが違いすぎることで、float32だと丸目誤差が10^-3程度でも起きるため、計算が不一致となる
    →log_scale_factorをtrainable=trueのtf.variableとして定義して、current_priceに乗算することで学習の対象とする
    →それではどうやら学習の対象にはならないので、.gradientで勾配計算するときとoptimizer.apply_gradientsを呼び出す時に明示的にgenerator.model.trainable_variables + [generator.log_scale_factor]とすることで、log_scale_factorを学習の対象としたいのだが、謎のエラーがapply_gradientsに対して出るので対処すべし
    →generatorのモデルの定義の後にself.log_scale_factorをadd_weightで定義して、前述のように.gradientと.apply_gradientsでlog_scale_factorを追加することで学習の対象とすることには成功
    →しかしなぜか、以前にも見られた、全てが0のposがpositionに追加されてしまう現象がまた発生している

    tensorarrayで保持しているposが計算グラフの情報を保持できているか確認せよ。具体的にはloss=add_required_marginとすることで、current_priceを使用しているadd_required_marginでgradientが計算できるか確認せよ
    →gen_gradient, disc_gradientsの両方で計算ができたことから、add_required_marginは、生成者の出力地であるcurrent_priceと識別者の出力値であるlong_order_size,short_order_size,long_close_position,short_close_positionの計算グラフの情報を保持していると言える

2025-2-4
    ※2025-1-23のところでも課題として挙げたが、process_new_orderをlongの後にshortを新規注文するように書いてあるがこれではlongの方が買いやすいというバイアスが生まれるのでだめ
    updated effective marginのところで、positions.writeとすると例えばpos_id=0であったposがpos_id=1になってpos_id=0のposとして全ての要素が0.0のposが追加されてしまう
    →pos_idをposから消したことでうまくいった。_remove_positionでgatherを使用したことで、pos_idが前倒しにずれることで、posの１つの要素としてpos_idを採用するとその値と、真のpos_idの値が不一致となる。

2025-2-6
    ※今は需給に関係なく、各エージェントが自分の資産の範囲内で売買したければ自分の意思だけでそれが可能な設定だが、実際には例えば買い注文をしてもそこで誰かが売ってなければ買うことはできない
    order_sizeそのものをlossとして勾配計算したところ、disc_gradientについて、なぜか1th agentだけが非負で、それ以外のagentが0のままだった。
    →order_sizeの更新に、effective_marginの更新にupdate_effective_marginを使用したように、new_order_sizeに一度更新した値を入れてからorder_size=new_order_sizeとする方法と、単純にpython演算子+=を使用する方法を試しましたが、lossをorder_sizeとした場合に依然として1th agentだけがdisc_gradientが非負だった。(seed=42)
    →seed=43に変更すると、1th agentは０となる一方で、2th, 3th agentが非負となった。よって重みの初期値で勾配消失が起きてしまっている。
    →order_sizeそのものをlossに使用して勾配消失が起きてしまっても、実際にはeffective_marginをlossに使用するだけなので問題なし。現在seed値を変えれば全てのagentが非負の勾配を持つことがわかっているのでorder_sizeが計算グラフの情報を保有していることを確認できたので問題解決です。

2025-2-9
    ※2025-2-2のところにも書いたが、全てが0のposがpositionに追加されてしまう現象が発生。
    ※log_scale_factorを学習の対象にすることには成功したが、学習の方向性の定義が課題。
